from __future__ import annotations

import time
import queue as pyqueue
import yaml
import cv2
import numpy as np
import multiprocessing as mp
from multiprocessing import shared_memory
from dataclasses import dataclass
from typing import Dict, Any, List, Tuple, Optional, Iterable, Set

import model.utils.tool as tool


@dataclass
class _Entry:
    disp_id: int
    last_seen: int
    born_seq: int


class DisplayIDManager:
    def __init__(self, max_ids: int = 40):
        self.max_ids = int(max_ids)
        self._free_ids = set(range(1, self.max_ids + 1))
        self._map: Dict[int, _Entry] = {}
        self._seq = 0

    def update(self, active_internal_ids: Iterable[int], frame_idx: int) -> None:
        active: Set[int] = set(int(x) for x in active_internal_ids)
        for iid in active:
            ent = self._map.get(iid)
            if ent is not None:
                ent.last_seen = int(frame_idx)
                continue
            disp_id = self._alloc_display_id(frame_idx=int(frame_idx))
            self._seq += 1
            self._map[iid] = _Entry(disp_id=disp_id, last_seen=int(frame_idx), born_seq=self._seq)

    def _alloc_display_id(self, frame_idx: int) -> int:
        if self._free_ids:
            disp_id = min(self._free_ids)
            self._free_ids.remove(disp_id)
            return disp_id
        victim_iid, victim_ent = self._select_victim(frame_idx)
        disp_id = victim_ent.disp_id
        del self._map[victim_iid]
        return disp_id

    def _select_victim(self, frame_idx: int) -> Tuple[int, _Entry]:
        best_iid: Optional[int] = None
        best_ent: Optional[_Entry] = None
        best_score = None
        for iid, ent in self._map.items():
            staleness = int(frame_idx) - int(ent.last_seen)
            score = (staleness, -ent.born_seq)
            if best_score is None or score > best_score:
                best_score = score
                best_iid = iid
                best_ent = ent
        return best_iid, best_ent

    def get_display_id(self, internal_id: int) -> Optional[int]:
        ent = self._map.get(int(internal_id))
        return None if ent is None else int(ent.disp_id)


CONFPATH = "model/conf/conf.yaml"


def load_conf(path: str) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def _cv2_runtime_tune():
    try:
        cv2.setNumThreads(1)
    except Exception:
        pass
    try:
        cv2.ocl.setUseOpenCL(False)
    except Exception:
        pass


def _shm_name(base: str, i: int) -> str:
    return f"{base}_{i}"


def _shm_open_buffers(base: str, n_slots: int, frame_bytes: int) -> List[shared_memory.SharedMemory]:
    bufs = []
    for i in range(n_slots):
        bufs.append(shared_memory.SharedMemory(name=_shm_name(base, i), create=False, size=frame_bytes))
    return bufs


def _shm_create_buffers(base: str, n_slots: int, frame_bytes: int) -> List[shared_memory.SharedMemory]:
    bufs = []
    for i in range(n_slots):
        bufs.append(shared_memory.SharedMemory(name=_shm_name(base, i), create=True, size=frame_bytes))
    return bufs


def video_decode_process(
    video_path: str,
    shm_base: str,
    n_slots: int,
    frame_h: int,
    frame_w: int,
    free_q: mp.Queue,
    infer_q: mp.Queue,
    disp_q: mp.Queue,
    refcounts: mp.Array,
    ref_lock: mp.Lock,
    stop_event: mp.Event,
    put_timeout: float = 0.2,
):
    _cv2_runtime_tune()

    frame_bytes = int(frame_h * frame_w * 3)
    shms = _shm_open_buffers(shm_base, n_slots, frame_bytes)

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise RuntimeError(f"Cannot open video: {video_path}")

    frame_idx = 0
    try:
        while not stop_event.is_set():
            ret, frame = cap.read()
            if not ret or frame is None:
                break

            if frame.ndim == 2:
                frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)
            elif frame.ndim == 3 and frame.shape[2] == 4:
                frame = cv2.cvtColor(frame, cv2.COLOR_BGRA2BGR)

            if frame.dtype != np.uint8:
                frame = np.clip(frame, 0, 255).astype(np.uint8)

            if frame.shape[0] != frame_h or frame.shape[1] != frame_w:
                frame = cv2.resize(frame, (frame_w, frame_h), interpolation=cv2.INTER_LINEAR)

            frame = np.ascontiguousarray(frame)

            while not stop_event.is_set():
                try:
                    slot = free_q.get(timeout=0.2)
                    break
                except pyqueue.Empty:
                    continue

            if stop_event.is_set():
                break

            mv = memoryview(shms[slot].buf)
            mv[:frame_bytes] = frame.reshape(-1).tobytes()

            with ref_lock:
                refcounts[slot] = 2

            item = (int(frame_idx), int(slot))

            while not stop_event.is_set():
                try:
                    infer_q.put(item, timeout=put_timeout)
                    break
                except pyqueue.Full:
                    continue

            while not stop_event.is_set():
                try:
                    disp_q.put(item, timeout=put_timeout)
                    break
                except pyqueue.Full:
                    continue

            frame_idx += 1

    finally:
        cap.release()
        try:
            infer_q.put(None, timeout=0.5)
        except Exception:
            pass
        try:
            disp_q.put(None, timeout=0.5)
        except Exception:
            pass
        for s in shms:
            try:
                s.close()
            except Exception:
                pass


class MainInfer:
    def __init__(self, yolo_weight: str, ckpt_path: Optional[str] = None):
        import torch
        import model.yolov7.yoloDetects2 as yoloDet
        import model.utils.modules.encoderAndHead as encoderAndHead
        from model.mainTracking import Tracking

        self.torch = torch
        self.device = (
            "cuda" if torch.cuda.is_available()
            else "mps" if hasattr(torch.backends, "mps") and torch.backends.mps.is_available()
            else "cpu"
        )

        self.conf = load_conf(CONFPATH)

        self.model = encoderAndHead.Model(
            in_channels=self.conf["yolo"]["in_channels"],
            out_channels=self.conf["yolo"]["out_channels"],
            warmup_epochs=10,
            proj_dim=128,
        ).to(self.device).eval()

        if str(self.device).startswith("cuda"):
            self.model.half()

        if ckpt_path is not None:
            ckpt = torch.load(ckpt_path, map_location="cpu")
            self.model.load_state_dict(ckpt["model"], strict=True)

        self.yolo = yoloDet.YoloDetects(
            weights=yolo_weight,
            conf_thres=self.conf["yolo"]["conf_thres"],
            iou_thres=self.conf["yolo"]["iou_thres"],
            img_size=self.conf["yolo"]["img_size"],
        )

        self.tracker = Tracking()

    def roi_align_from_input_boxes(
        self,
        feat,
        boxes_in: List[List[float]],
        input_hw: Tuple[int, int],
        out_size=(7, 7),
        aligned=True,
        sampling_ratio=2,
    ):
        import torch
        from torchvision.ops import roi_align

        H_in, W_in = input_hw
        _, _, Hf, Wf = feat.shape
        spatial_scale = Hf / float(H_in)

        rois = torch.tensor(
            [[0.0, b[0], b[1], b[2], b[3]] for b in boxes_in],
            dtype=feat.dtype,
            device=feat.device,
        )
        return roi_align(
            input=feat,
            boxes=rois,
            output_size=out_size,
            spatial_scale=spatial_scale,
            sampling_ratio=sampling_ratio,
            aligned=aligned,
        )


def inference_process(
    shm_base: str,
    n_slots: int,
    frame_h: int,
    frame_w: int,
    infer_q: mp.Queue,
    res_q: mp.Queue,
    free_q: mp.Queue,
    refcounts: mp.Array,
    ref_lock: mp.Lock,
    stop_event: mp.Event,
    conf_path: str = CONFPATH,
    queue_get_timeout: float = 0.2,
    min_conf: float = 0.01,
):
    _cv2_runtime_tune()

    import torch
    import torch.nn.functional as F

    conf = load_conf(conf_path)
    infer = MainInfer(
        yolo_weight=conf["model"]["yolo_weight"],
        ckpt_path=conf["model"]["encoder_weight"],
    )

    frame_bytes = int(frame_h * frame_w * 3)
    shms = _shm_open_buffers(shm_base, n_slots, frame_bytes)
    frame_np = np.empty((frame_h, frame_w, 3), dtype=np.uint8)

    while not stop_event.is_set():
        try:
            item = infer_q.get(timeout=queue_get_timeout)
        except pyqueue.Empty:
            continue

        if item is None:
            break

        frame_idx, slot = int(item[0]), int(item[1])

        mv = memoryview(shms[slot].buf)
        frame_np.reshape(-1)[:] = np.frombuffer(mv[:frame_bytes], dtype=np.uint8, count=frame_bytes)

        bbox_info, _, feat = infer.yolo.run_with_tensor(frame_np, return_img_tensor=True)

        if (not bbox_info) or (feat is None):
            obj = {
                "embs": [],
                "bboxes": [],
                "confs": [],
                "input_hw": (frame_h, frame_w),
                "frame_id": int(frame_idx),
            }
            infer.tracker.update(obj)
            res_q.put((frame_idx, {"boxes_xyxy": [], "confs": [], "assignments": []}))
        else:
            boxes_in: List[List[float]] = []
            confs: List[float] = []
            boxes_xyxy: List[List[int]] = []

            for d in bbox_info:
                c = float(d.get("conf", 0.0))
                if c < float(min_conf):
                    continue

                b_in = d.get("xyxy_in", None)
                if b_in is None or len(b_in) != 4:
                    continue

                boxes_in.append([float(x) for x in b_in])
                confs.append(c)

                if "xyxy" in d and d["xyxy"] is not None and len(d["xyxy"]) == 4:
                    x1, y1, x2, y2 = d["xyxy"]
                    boxes_xyxy.append([int(x1), int(y1), int(x2), int(y2)])
                else:
                    cx, cy, bw, bh = float(d["x"]), float(d["y"]), float(d["w"]), float(d["h"])
                    x1 = int(cx - bw / 2.0)
                    y1 = int(cy - bh / 2.0)
                    x2 = int(cx + bw / 2.0)
                    y2 = int(cy + bh / 2.0)
                    boxes_xyxy.append([x1, y1, x2, y2])

            if len(boxes_in) == 0:
                obj = {
                    "embs": [],
                    "bboxes": [],
                    "confs": [],
                    "input_hw": (frame_h, frame_w),
                    "frame_id": int(frame_idx),
                }
                infer.tracker.update(obj)
                res_q.put((frame_idx, {"boxes_xyxy": [], "confs": [], "assignments": []}))
            else:
                input_hw = tuple(bbox_info[0]["input_hw"])

                roi = infer.roi_align_from_input_boxes(
                    feat=feat,
                    boxes_in=boxes_in,
                    input_hw=input_hw,
                    out_size=(7, 7),
                )

                with torch.no_grad():
                    emb_cur = infer.model(roi)
                emb_cur = F.normalize(emb_cur.float(), dim=-1)

                embs_np = emb_cur.detach().cpu().numpy().astype(np.float32)
                embs_list = [embs_np[i].reshape(-1) for i in range(embs_np.shape[0])]

                obj = {
                    "embs": embs_list,
                    "bboxes": boxes_in,
                    "confs": confs,
                    "input_hw": tuple(input_hw),
                    "frame_id": int(frame_idx),
                }

                matches_tid, _, _ = infer.tracker.update(obj)
                assignments = [{"track_id": int(tid), "det_idx": int(det_j)} for (tid, det_j) in (matches_tid or [])]

                payload = {
                    "boxes_xyxy": boxes_xyxy,
                    "confs": confs,
                    "assignments": assignments,
                }
                res_q.put((frame_idx, payload))

        with ref_lock:
            refcounts[slot] -= 1
            if refcounts[slot] <= 0:
                refcounts[slot] = 0
                try:
                    free_q.put_nowait(slot)
                except Exception:
                    pass

    try:
        res_q.put(None, timeout=0.5)
    except Exception:
        pass

    for s in shms:
        try:
            s.close()
        except Exception:
            pass


def track(
    video_path: str,
    out_path: Optional[str] = None,
    queue_size: int = 8,
    show_window: bool = True,
    max_ids: int = 40,
    target_size: Tuple[int, int] = (1280, 720),
):
    mp.set_start_method("spawn", force=True)
    _cv2_runtime_tune()

    frame_w, frame_h = int(target_size[0]), int(target_size[1])
    n_slots = int(max(2, min(32, queue_size)))
    shm_base = f"trk_shm_{int(time.time() * 1e6)}"
    frame_bytes = int(frame_h * frame_w * 3)

    shms = _shm_create_buffers(shm_base, n_slots, frame_bytes)

    free_q: mp.Queue = mp.Queue(maxsize=n_slots)
    infer_q: mp.Queue = mp.Queue(maxsize=n_slots)
    disp_q: mp.Queue = mp.Queue(maxsize=n_slots)
    res_q: mp.Queue = mp.Queue(maxsize=n_slots)

    for i in range(n_slots):
        free_q.put(i)

    stop_event: mp.Event = mp.Event()
    refcounts = mp.Array("i", [0] * n_slots, lock=False)
    ref_lock = mp.Lock()

    p_decode = mp.Process(
        target=video_decode_process,
        args=(video_path, shm_base, n_slots, frame_h, frame_w, free_q, infer_q, disp_q, refcounts, ref_lock, stop_event, 0.2),
        name="video_decode",
        daemon=True,
    )

    p_infer = mp.Process(
        target=inference_process,
        args=(shm_base, n_slots, frame_h, frame_w, infer_q, res_q, free_q, refcounts, ref_lock, stop_event, CONFPATH, 0.2, 0.01),
        name="inference",
        daemon=False,
    )

    p_decode.start()
    p_infer.start()

    if show_window:
        WIN = "tracking"
        cv2.namedWindow(WIN, cv2.WINDOW_NORMAL)
        cv2.resizeWindow(WIN, 1280, 720)

    res_buf: Dict[int, Dict[str, Any]] = {}

    def drain_results(max_items: int = 256) -> bool:
        cnt = 0
        while cnt < max_items:
            try:
                r = res_q.get_nowait()
            except pyqueue.Empty:
                break
            if r is None:
                return False
            fidx, payload = r
            res_buf[int(fidx)] = payload
            cnt += 1
        return True

    mon = tool.ResourceMonitor(gpu_index=0, sample_interval=0.2).start()
    all_gpu: List[float] = []
    all_cpu: List[float] = []

    from tqdm import tqdm

    frame_np = np.empty((frame_h, frame_w, 3), dtype=np.uint8)

    try:
        id_manager = DisplayIDManager(max_ids=max_ids)
        with tqdm(total=None, desc="Tracking", unit="frame", dynamic_ncols=True) as pbar:
            while True:
                alive = drain_results()
                if not alive:
                    pass

                try:
                    item = disp_q.get(timeout=0.2)
                except pyqueue.Empty:
                    if not p_infer.is_alive() and disp_q.empty():
                        break
                    continue

                if item is None:
                    break

                frame_idx, slot = int(item[0]), int(item[1])

                mv = memoryview(shms[slot].buf)
                frame_np.reshape(-1)[:] = np.frombuffer(mv[:frame_bytes], dtype=np.uint8, count=frame_bytes)

                payload = res_buf.pop(frame_idx, None)
                t0 = time.time()
                while payload is None and (time.time() - t0) < 0.2:
                    drain_results(max_items=512)
                    payload = res_buf.pop(frame_idx, None)
                    if payload is not None:
                        break
                    time.sleep(0.001)

                if payload is not None:
                    boxes_xyxy = payload.get("boxes_xyxy", [])
                    confs = payload.get("confs", [])
                    assignments = payload.get("assignments", [])

                    for i, (x1, y1, x2, y2) in enumerate(boxes_xyxy):
                        c = float(confs[i]) if i < len(confs) else 0.0
                        cv2.rectangle(frame_np, (x1, y1), (x2, y2), (255, 255, 0), 2)
                        cv2.putText(
                            frame_np,
                            f"D{i}:{c:.2f}",
                            (x1, max(0, y1 - 5)),
                            cv2.FONT_HERSHEY_SIMPLEX,
                            0.8,
                            (0, 0, 0),
                            2,
                        )

                    active_ids = [a["track_id"] for a in assignments]
                    id_manager.update(active_ids, frame_idx)

                    for a in assignments:
                        internal_id = int(a["track_id"])
                        det_idx = int(a["det_idx"])
                        display_id = id_manager.get_display_id(internal_id)
                        if display_id is None:
                            continue
                        if det_idx < 0 or det_idx >= len(boxes_xyxy):
                            continue
                        x1, y1, x2, y2 = boxes_xyxy[det_idx]
                        c = float(confs[det_idx]) if det_idx < len(confs) else 0.0
                        cv2.rectangle(frame_np, (x1, y1), (x2, y2), (0, 255, 0), 2)
                        cv2.putText(
                            frame_np,
                            f"ID:{display_id} Conf:{c:.2f}",
                            (x1, max(0, y1 - 28)),
                            cv2.FONT_HERSHEY_SIMPLEX,
                            1,
                            (0, 0, 0),
                            3,
                        )

                    pbar.set_postfix(
                        det=len(boxes_xyxy),
                        cpu=f"{mon.cpu_util:.0f}%",
                        gpu=f"{mon.gpu_util:.0f}%",
                    )
                    all_gpu.append(mon.gpu_util)
                    all_cpu.append(mon.cpu_util)
                else:
                    pbar.set_postfix(det="-")

                if show_window:
                    cv2.imshow("tracking", frame_np)
                    if cv2.waitKey(1) & 0xFF == ord("q"):
                        stop_event.set()
                        try:
                            infer_q.put_nowait(None)
                        except Exception:
                            pass
                        try:
                            disp_q.put_nowait(None)
                        except Exception:
                            pass
                        try:
                            res_q.put_nowait(None)
                        except Exception:
                            pass
                        break

                with ref_lock:
                    refcounts[slot] -= 1
                    if refcounts[slot] <= 0:
                        refcounts[slot] = 0
                        try:
                            free_q.put_nowait(slot)
                        except Exception:
                            pass

                pbar.update(1)

    finally:
        stop_event.set()
        mon.close()
        if show_window:
            cv2.destroyAllWindows()

        if p_infer.is_alive():
            p_infer.join(timeout=2.0)
        if p_decode.is_alive():
            p_decode.join(timeout=2.0)

        for s in shms:
            try:
                s.close()
            except Exception:
                pass
            try:
                s.unlink()
            except Exception:
                pass

        if all_cpu and all_gpu:
            print(f"[Res]: avg cpu {sum(all_cpu)/len(all_cpu):.2f}% , avg gpu {sum(all_gpu)/len(all_gpu):.2f}%")
            print(f"[Res]: max cpu {max(all_cpu):.2f}% , max gpu {max(all_gpu):.2f}%")


if __name__ == "__main__":
    video_path = "video/car.mp4" # PATH TO YOUR VIDEO FILE
    track(video_path, out_path=None, queue_size=8, show_window=True, max_ids=40, target_size=(1280, 720))
